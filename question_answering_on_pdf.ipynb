{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50aed7a",
   "metadata": {},
   "source": [
    "# Question answering on Private Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7361d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2c409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain_community pypdf docx2txt wikipedia langchain-text-splitters tiktoken langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbedde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    \n",
    "    import os \n",
    "    name, extension = os.path.splitext(file)\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        # for pdf\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "        \n",
    "    elif extension == \".docx\":\n",
    "        # for word doc\n",
    "        from langchain_community.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported')\n",
    "        return None\n",
    "        \n",
    "    # return list of langchain doc (1 doc for each page)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "# wikipedia \n",
    "# query: text find doc in wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "    \n",
    "    # load_max_docs is to limit number of downloaded documents\n",
    "    docs = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs).load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf3a14",
   "metadata": {},
   "source": [
    "- chunk_size: The maximum size of a chunk, where size is determined by the length_function.\n",
    "- chunk_overlap: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de372b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size = 256):\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    # automatically split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "    # return list of data\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3809d",
   "metadata": {},
   "source": [
    "# Calculating Cost of openAI embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df963cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e319130",
   "metadata": {},
   "source": [
    "# Embedding and Uploading to Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    # importing the necessary libraries and initializing the Pinecone client\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if pc.has_index(index_name):\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        # creating the index and embedding the chunks into the index \n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "\n",
    "        # creating a new index\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ) \n",
    "        )\n",
    "\n",
    "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "        # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30267a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    from pinecone import Pinecone\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        # this delete all the indexes\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ... ')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cd62c",
   "metadata": {},
   "source": [
    "# Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    # vector store: source of knowledge\n",
    "    # q: question\n",
    "    # 1. Retrieve most relevant chunk from vector db for question\n",
    "    # 2. Feed chunk to llm to get final answer in natural language\n",
    "    \n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=1)\n",
    "    \n",
    "    # search for 3 most relevant chunks of information\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e3dd9",
   "metadata": {},
   "source": [
    "## Running Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5fefe",
   "metadata": {},
   "source": [
    "#### Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f73d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "There are 1173 characters\n"
     ]
    }
   ],
   "source": [
    "data = load_document(\"files/us_constitution.pdf\")\n",
    "# second page\n",
    "# print(data[1].page_content[:100])\n",
    "# print(data[1].metadata)\n",
    "# print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275ae6d",
   "metadata": {},
   "source": [
    "#### Loading word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145b6c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/the_great_gatsby.docx\n",
      "The Project Gutenberg eBook of The Great Gatsby, by F. Scott Fitzgerald\n",
      "\n",
      "\n",
      "\n",
      "This eBook is for the use\n"
     ]
    }
   ],
   "source": [
    "data = load_document(\"files/the_great_gatsby.docx\")\n",
    "print(data[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d904e",
   "metadata": {},
   "source": [
    "#### Loading wikipedia info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e3c3699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Hunter × Hunter',\n",
       " 'summary': 'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.',\n",
       " 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_wikipedia(query=\"HUNTER X HUNTER\")\n",
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f100b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy nam'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk document (embed with little noise possible while keeping sematically relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e7c498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "Blessings of Liberty to ourselves and our Posterity , do ordain and \n",
      " establish this Constitution for the United States of America. \n",
      "The Constitutional Con v ention \n",
      " Article I \n",
      " Section 1: Congress\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5002ae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 9842\n",
      "Embedding Cost in USD: 0.000197\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642bea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
